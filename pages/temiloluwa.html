<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>3DHAR-AI</title>
    <link
      href="https://fonts.googleapis.com/css2?family=Orbitron:wght@400;700;900&family=Rajdhani:wght@300;400;500;700&display=swap"
      rel="stylesheet"
    />
    <link href="../styles.css" rel="stylesheet" />
  </head>
  <body>
    <!-- Animated Grid Background -->
    <div class="grid-bg"></div>
    <div class="gradient-overlay"></div>
    <div class="scanlines"></div>

    <!-- Animated Shapes -->
    <div class="shapes-container">
      <div class="shape shape-circle"></div>
      <div class="shape shape-triangle"></div>
      <div class="shape shape-square"></div>
    </div>

    <!-- Floating Particles -->
    <div id="particles"></div>

    <!-- Navigation -->
    <nav id="navbar">
      <div class="nav-container">
        <a href="#home" class="logo-link">
          <svg
            class="logo-svg"
            viewBox="0 0 40 40"
            xmlns="http://www.w3.org/2000/svg"
          >
            <defs>
              <linearGradient id="logoGradient" x1="0%" y1="0%" x2="100%" y2="100%">
                <stop offset="0%" style="stop-color: #0b59de; stop-opacity: 1" />
                <stop offset="100%" style="stop-color: #f2f2f2; stop-opacity: 1" />
              </linearGradient>
            </defs>
            <polygon
              points="20,2 38,14 38,26 20,38 2,26 2,14"
              fill="none"
              stroke="url(#logoGradient)"
              stroke-width="2"
            />
            <polygon
              points="20,8 32,16 32,24 20,32 8,24 8,16"
              fill="url(#logoGradient)"
              opacity="0.3"
            />
            <circle cx="20" cy="20" r="3" fill="url(#logoGradient)" />
          </svg>
          <span class="logo-text">3DHAR-AI</span>
        </a>
        <ul class="nav-links" id="navLinks">
          <li><a href="../index.html" class="nav-link">Return Home</a></li>
          <li><a href="#insights" class="nav-link">Contributions</a></li>
        </ul>
        <div class="menu-toggle" id="menuToggle">
          <span></span>
          <span></span>
          <span></span>
        </div>
      </div>
    </nav>

    <!-- Hero Section -->
    <section class="hero" id="home">
      <div class="hero-content">
        <div class="text-rotator">
          <div class="text-set active">
            <h1 class="glitch-text" data-text="Welcome">Welcome</h1>
            <p class="subtitle">Temiloluwa's Contributions to 3DHAR-AI</p>
          </div>
        </div>
      </div>
      <div class="cta-container">
        <a href="#insights" class="cta-button cta-primary">Learn More</a>
        <a href="../index.html" class="cta-button cta-secondary">Return Home</a>
      </div>
    </section>

    <!-- Insights Section -->
    <section class="insights" id="insights">
      <h2 class="section-title">Core Contributions</h2>
      <div class="insights-container">
        <div class="insights-tabs">
          <div class="tab-item active" data-tab="introduction">
            <span class="tab-icon">‚ö°</span><span>Introduction</span>
          </div>
          <div class="tab-item" data-tab="methodology">
            <span class="tab-icon">üî¨</span><span>Methodology</span>
          </div>
          <div class="tab-item" data-tab="stgnn">
            <span class="tab-icon">üåê</span><span>STGNNs</span>
          </div>
          <div class="tab-item" data-tab="time-encoding">
            <span class="tab-icon">‚è±Ô∏è</span><span>Time Encoding</span>
          </div>
          <div class="tab-item" data-tab="results">
            <span class="tab-icon">üìä</span><span>Results</span>
          </div>
        </div>

        <!-- Content Panels -->
        <div class="insights-content">
          <div class="content-panel active" id="introduction">
            <h3>Introduction</h3>
            <p>
              Human Activity Recognition (HAR) and Prediction (HAP) enable intelligent
              smart home systems that can monitor, assist, and adapt to human behavior.
              Using motion, door, and temperature sensors, these systems interpret
              daily routines such as cooking, relaxing, and sleeping.
            </p>
            <p>
              Traditional deep learning methods like CNNs and LSTMs perform well on
              sensor-based HAR but fail to capture explicit spatial relationships between
              sensors. Spatial-Temporal Graph Neural Networks (STGNNs) address this
              limitation by jointly modeling both spatial dependencies and temporal
              dynamics. This work investigates whether STGNNs, specifically Graph
              WaveNet and AGCRN, can outperform conventional baselines.
            </p>
          </div>

          <div class="content-panel" id="methodology">
            <h3>Methodology</h3>
            <p>
              Two public smart home datasets from the CASAS project ‚Äî <strong>Aruba</strong>
              and <strong>Milan</strong> ‚Äî were used. These datasets include motion, door,
              and temperature sensors with annotated daily activities.
            </p>
            <p>
              Preprocessing involved cleaning anomalies, assigning activities to sensor
              readings, and converting the event-driven data into structured windows.
              Each sample was represented as a 4D tensor:
              <code>(windows √ó steps √ó sensors √ó features)</code>.
            </p>
            <p>
              Models were evaluated using 3-fold walk-forward validation to preserve
              temporal order, and the weighted F1-score was used as the primary metric.
            </p>
          </div>

          <div class="content-panel" id="stgnn">
            <h3>Spatial-Temporal Graph Neural Networks</h3>
            <p>
              Two STGNN architectures were implemented:
            </p>
            <ul class="insights-list">
              <li>
                <strong>Graph WaveNet (GWN):</strong> Combines graph convolutions with
                temporal convolutions. Learns an adaptive adjacency matrix, allowing
                sensor relationships to be inferred directly from data.
              </li>
              <li>
                <strong>Adaptive Graph Convolutional Recurrent Network (AGCRN):</strong>
                Integrates graph convolutions within recurrent units, learning
                both spatial and temporal patterns. It includes modules that adaptively
                generate graph structures and node-specific parameters.
              </li>
            </ul>
            <p>
              Both models aim to capture complex sensor dependencies that traditional
              architectures overlook.
            </p>
          </div>

          <div class="content-panel" id="time-encoding">
            <h3>Time Encoding</h3>
            <p>
              Smart home sensor data is irregular and event-driven. Instead of sampling
              at fixed intervals, we engineered time-based features to encode the passage
              of time explicitly.
            </p>
            <ul class="insights-list">
              <li>Œît<sub>global</sub>: Time since the last event from any sensor</li>
              <li>Œît<sub>sensor</sub>: Time since the last activation of the same sensor</li>
              <li>Sensor value: Encoded as -1 or 1 depending on state</li>
            </ul>
            <p>
              All time deltas were log-normalized to reduce skew and compress large
              gaps. This representation improved model stability and significantly
              increased performance compared to fixed-interval sampling.
            </p>
          </div>

          <div class="content-panel" id="results">
            <h3>Results</h3>
            <p>
              On the <strong>Aruba dataset</strong>, the AGCRN and LSTM achieved the highest
              weighted F1-scores (0.82), while Graph WaveNet performed best with
              shorter windows. On the more challenging <strong>Milan dataset</strong>,
              all models performed similarly (‚âà0.53 F1), reflecting greater variability
              and noise.
            </p>
            <p>
              For Human Activity Prediction (HAP), performance declined across all
              models due to increased task difficulty. Nevertheless, STGNNs learned
              meaningful inter-room transitions, demonstrating interpretability
              advantages.
            </p>
            <p>
              Overall, STGNNs offered comparable accuracy to CNN and LSTM baselines,
              with added transparency in their learned graph structures ‚Äî a promising
              step toward trustworthy smart home AI.
            </p>
          </div>
        </div>
      </div>
    </section>

    <!-- Footer -->
    <footer>
      <div class="footer-content">
        <p class="copyright">
          ¬© 2025 3DHAR-AI | Design by
          <a href="https://templatemo.com" target="_blank" rel="nofollow noopener"
            >TemplateMo</a
          >
        </p>
      </div>
    </footer>
    <script src="../scripts.js"></script>
  </body>
</html>
